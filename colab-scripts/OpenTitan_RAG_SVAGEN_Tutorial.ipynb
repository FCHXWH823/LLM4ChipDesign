{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64c7744",
   "metadata": {},
   "source": [
    "# OpenTitan RAG-based SystemVerilog Assertion Generator\n",
    "\n",
    "This notebook examines a system that combines web scraping, semantic search, and large language models to automatically generate assertions for hardware IP verification.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand how Retrieval-Augmented Generation (RAG) applies to hardware verification\n",
    "- Examine SystemVerilog Assertion generation using LLMs\n",
    "- Analyze web scraping and document processing for hardware documentation\n",
    "- Study semantic search with vector embeddings\n",
    "\n",
    "### Prerequisites\n",
    "- Basic understanding of Python programming\n",
    "- Familiarity with hardware design and verification concepts\n",
    "- Knowledge of SystemVerilog (helpful but not required)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e94a6",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "This section establishes the required dependencies and clones the repository for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df26f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning repository from https://github.com/AnandMenon12/OpenTitan_RAG_SVAGEN.git...\n",
      "Repository cloned successfully!\n",
      "Changed to directory: /home/eng/a/axm200085/Transformer_annotated/CollabTest/OpenTitan_RAG_SVAGEN/OpenTitan_RAG_SVAGEN\n",
      "\n",
      "Repository contents:\n",
      "  .git/\n",
      "  .gitignore (585 bytes)\n",
      "  README.md (3586 bytes)\n",
      "  opentitan_sva_generator.py (20254 bytes)\n",
      "  requirements.txt (208 bytes)\n",
      "Repository cloned successfully!\n",
      "Changed to directory: /home/eng/a/axm200085/Transformer_annotated/CollabTest/OpenTitan_RAG_SVAGEN/OpenTitan_RAG_SVAGEN\n",
      "\n",
      "Repository contents:\n",
      "  .git/\n",
      "  .gitignore (585 bytes)\n",
      "  README.md (3586 bytes)\n",
      "  opentitan_sva_generator.py (20254 bytes)\n",
      "  requirements.txt (208 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Clone the OpenTitan RAG SVAGEN repository\n",
    "import subprocess\n",
    "\n",
    "repo_url = \"https://github.com/AnandMenon12/OpenTitan_RAG_SVAGEN.git\"\n",
    "repo_name = \"OpenTitan_RAG_SVAGEN\"\n",
    "\n",
    "# Check if repository already exists\n",
    "if not os.path.exists(repo_name):\n",
    "    print(f\"Cloning repository from {repo_url}...\")\n",
    "    result = subprocess.run([\"git\", \"clone\", repo_url], capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"Repository cloned successfully\")\n",
    "    else:\n",
    "        print(f\"Error cloning repository: {result.stderr}\")\n",
    "else:\n",
    "    print(\"Repository already exists\")\n",
    "\n",
    "# Change to the repository directory\n",
    "os.chdir(repo_name)\n",
    "print(f\"Changed to directory: {os.getcwd()}\")\n",
    "\n",
    "# List the contents of the repository\n",
    "print(\"\\nRepository contents:\")\n",
    "for item in os.listdir('.'):\n",
    "    if os.path.isfile(item):\n",
    "        size = os.path.getsize(item)\n",
    "        print(f\"  {item} ({size} bytes)\")\n",
    "    else:\n",
    "        print(f\"  {item}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134918ef",
   "metadata": {},
   "source": [
    "## 2. Code Structure Analysis\n",
    "\n",
    "The following analysis examines the main components and dependencies of the OpenTitan RAG SVAGEN system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d91e9828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirements from requirements.txt:\n",
      "# Requirements for OpenTitan SVA Generator\n",
      "torch>=2.0.0\n",
      "transformers>=4.35.0\n",
      "sentence-transformers>=2.2.2\n",
      "faiss-cpu>=1.7.4\n",
      "numpy>=1.21.0\n",
      "markdown>=3.4.0\n",
      "beautifulsoup4>=4.11.0\n",
      "hjson>=3.1.0\n",
      "accelerate>=0.21.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's examine the requirements.txt file if it exists\n",
    "if os.path.exists('requirements.txt'):\n",
    "    print(\"Requirements from requirements.txt:\")\n",
    "    with open('requirements.txt', 'r') as f:\n",
    "        requirements = f.read()\n",
    "        print(requirements)\n",
    "else:\n",
    "    print(\"No requirements.txt found. We'll install dependencies manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73432a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "Installing torch...\n",
      "  torch installed successfully\n",
      "Installing transformers...\n",
      "  torch installed successfully\n",
      "Installing transformers...\n",
      "  transformers installed successfully\n",
      "Installing sentence-transformers...\n",
      "  transformers installed successfully\n",
      "Installing sentence-transformers...\n",
      "  sentence-transformers installed successfully\n",
      "Installing faiss-cpu...\n",
      "  sentence-transformers installed successfully\n",
      "Installing faiss-cpu...\n",
      "  faiss-cpu installed successfully\n",
      "Installing beautifulsoup4...\n",
      "  faiss-cpu installed successfully\n",
      "Installing beautifulsoup4...\n",
      "  beautifulsoup4 installed successfully\n",
      "Installing requests...\n",
      "  beautifulsoup4 installed successfully\n",
      "Installing requests...\n",
      "  requests installed successfully\n",
      "Installing numpy...\n",
      "  requests installed successfully\n",
      "Installing numpy...\n",
      "  numpy installed successfully\n",
      "Installing markdown...\n",
      "  numpy installed successfully\n",
      "Installing markdown...\n",
      "  markdown installed successfully\n",
      "Installing hjson...\n",
      "  markdown installed successfully\n",
      "Installing hjson...\n",
      "  hjson installed successfully\n",
      "Installing lxml...\n",
      "  hjson installed successfully\n",
      "Installing lxml...\n",
      "  lxml installed successfully\n",
      "\n",
      "Dependency installation complete!\n",
      "  lxml installed successfully\n",
      "\n",
      "Dependency installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install the core dependencies\n",
    "# Note: In practice, you would install from requirements.txt, but we'll install manually for educational purposes\n",
    "\n",
    "dependencies = [\n",
    "    \"torch\",                    # PyTorch for deep learning\n",
    "    \"transformers\",            # Hugging Face transformers for LLMs\n",
    "    \"sentence-transformers\",   # For text embeddings\n",
    "    \"faiss-cpu\",              # Facebook AI Similarity Search\n",
    "    \"beautifulsoup4\",          # HTML/XML parsing\n",
    "    \"requests\",               # HTTP requests\n",
    "    \"numpy\",                  # Numerical computing\n",
    "    \"markdown\",               # Markdown processing\n",
    "    \"hjson\",                  # Human JSON for hardware configs\n",
    "    \"lxml\"                    # XML processing\n",
    "]\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "for dep in dependencies:\n",
    "    print(f\"Installing {dep}...\")\n",
    "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", dep], \n",
    "                          capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"  {dep} installed successfully\")\n",
    "    else:\n",
    "        print(f\"  Error installing {dep}: {result.stderr[:200]}...\")\n",
    "\n",
    "print(\"\\nDependency installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d737e29",
   "metadata": {},
   "source": [
    "## 3. Data Structures Analysis\n",
    "\n",
    "The system employs several dataclasses to represent different types of documentation elements. These structures provide type safety and clear interfaces for data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5523eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source code statistics:\n",
      "  - Total lines: 525\n",
      "  - Total characters: 20254\n",
      "  - File size: 20254 bytes\n",
      "\n",
      "Classes defined: ['DocumentChunk', 'RTLSignal', 'RegisterField', 'OpenTitanIngester', 'EmbeddingManager', 'SVAGenerator', 'OpenTitanSVASystem']\n",
      "Data classes: ['DocumentChunk', 'RTLSignal', 'RegisterField']\n"
     ]
    }
   ],
   "source": [
    "# Extract and analyze dataclasses from the source code\n",
    "dataclasses = []\n",
    "lines = source_code.split('\\n')\n",
    "\n",
    "# Find dataclass definitions\n",
    "for i, line in enumerate(lines):\n",
    "    if line.strip().startswith('@dataclass'):\n",
    "        class_line = lines[i+1]\n",
    "        class_match = re.search(r'class (\\w+)', class_line)\n",
    "        if class_match:\n",
    "            dataclasses.append(class_match.group(1))\n",
    "\n",
    "print(\"Identified Dataclasses:\")\n",
    "for dc in dataclasses:\n",
    "    print(f\"- {dc}\")\n",
    "\n",
    "# Extract the complete dataclass code for analysis\n",
    "print(\"\\nDataclass Implementations:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# DocumentChunk dataclass\n",
    "dc_match = re.search(r'@dataclass\\nclass DocumentChunk:.*?(?=@dataclass|\\nclass [A-Z]|\\Z)', source_code, re.DOTALL)\n",
    "if dc_match:\n",
    "    print(\"DocumentChunk:\")\n",
    "    print(dc_match.group(0)[:300] + \"...\")\n",
    "\n",
    "# RTLSignal dataclass  \n",
    "rtl_match = re.search(r'@dataclass\\nclass RTLSignal:.*?(?=@dataclass|\\nclass [A-Z]|\\Z)', source_code, re.DOTALL)\n",
    "if rtl_match:\n",
    "    print(\"\\nRTLSignal:\")\n",
    "    print(rtl_match.group(0)[:300] + \"...\")\n",
    "\n",
    "# RegisterField dataclass\n",
    "reg_match = re.search(r'@dataclass\\nclass RegisterField:.*?(?=@dataclass|\\nclass [A-Z]|\\Z)', source_code, re.DOTALL)\n",
    "if reg_match:\n",
    "    print(\"\\nRegisterField:\")\n",
    "    print(reg_match.group(0)[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9802c934",
   "metadata": {},
   "source": [
    "## 4. OpenTitanIngester Class\n",
    "\n",
    "The OpenTitanIngester class handles web scraping of OpenTitan documentation. It processes web pages to extract relevant documentation chunks for the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining the key data structures...\n",
      "\n",
      " 1: import os\n",
      " 2: import re\n",
      " 3: import json\n",
      " 4: import pickle\n",
      " 5: from pathlib import Path\n",
      " 6: from typing import Dict, List, Tuple, Optional, Any\n",
      " 7: from dataclasses import dataclass\n",
      " 8: from datetime import datetime\n",
      " 9: \n",
      "10: import requests\n",
      "11: from bs4 import BeautifulSoup\n",
      "12: import markdown\n",
      "13: import numpy as np\n",
      "14: import faiss\n",
      "15: from sentence_transformers import SentenceTransformer\n",
      "16: from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "17: import torch\n",
      "18: \n",
      "19: @dataclass\n",
      "20: class DocumentChunk:\n",
      "21:     \"\"\"Represents a chunk of documentation with metadata\"\"\"\n",
      "22:     content: str\n",
      "23:     ip_name: str\n",
      "24:     file_path: str\n",
      "25:     section: str\n",
      "26:     signal_refs: List[str]\n",
      "27:     fsm_state_refs: List[str]\n",
      "28:     chunk_type: str  # 'doc', 'register', 'rtl'\n",
      "29:     metadata: Dict[str, Any]\n",
      "30: \n",
      "31: @dataclass\n",
      "32: class RTLSignal:\n",
      "33:     \"\"\"Represents an RTL signal or port\"\"\"\n",
      "34:     name: str\n",
      "35:     width: int\n",
      "36:     direction: str  # input, output, internal\n",
      "37:     module: str\n",
      "38:     reset_value: Optional[str] = None\n",
      "39:     description: Optional[str] = None\n",
      "40: \n",
      "41: @dataclass\n",
      "42: class RegisterField:\n",
      "43:     \"\"\"Represents a register field from HJSON\"\"\"\n",
      "44:     name: str\n",
      "45:     address: str\n",
      "46:     reset_value: str\n",
      "47:     access: str\n",
      "48:     brief: str\n",
      "49:     register_name: str\n",
      "50: \n"
     ]
    }
   ],
   "source": [
    "# Analyze the OpenTitanIngester class\n",
    "ingester_match = re.search(r'class OpenTitanIngester:.*?(?=class [A-Z]|\\Z)', source_code, re.DOTALL)\n",
    "\n",
    "if ingester_match:\n",
    "    ingester_code = ingester_match.group(0)\n",
    "    \n",
    "    print(\"OpenTitanIngester Class Analysis:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Find methods\n",
    "    methods = re.findall(r'def (\\w+)\\(', ingester_code)\n",
    "    print(\"Methods:\")\n",
    "    for method in methods:\n",
    "        print(f\"- {method}\")\n",
    "    \n",
    "    # Show class structure\n",
    "    lines = ingester_code.split('\\n')\n",
    "    print(f\"\\nClass structure ({len(lines)} lines of code)\")\n",
    "    \n",
    "    # Show the __init__ method\n",
    "    init_match = re.search(r'def __init__\\(.*?\\):(.*?)(?=def|\\Z)', ingester_code, re.DOTALL)\n",
    "    if init_match:\n",
    "        print(\"\\nInitialization parameters:\")\n",
    "        init_lines = init_match.group(0).split('\\n')[:10]\n",
    "        for line in init_lines:\n",
    "            if line.strip():\n",
    "                print(f\"  {line.strip()}\")\n",
    "    \n",
    "    # Show scraping method signature\n",
    "    scrape_match = re.search(r'def scrape_documentation\\([^)]*\\):', ingester_code)\n",
    "    if scrape_match:\n",
    "        print(f\"\\nMain scraping method: {scrape_match.group(0)}\")\n",
    "else:\n",
    "    print(\"OpenTitanIngester class not found in source code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f9025",
   "metadata": {},
   "source": [
    "## 5. EmbeddingManager Class\n",
    "\n",
    "The EmbeddingManager handles vector embeddings and semantic search functionality. It creates and manages a FAISS index for efficient similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c27310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenTitanIngester class structure:\n",
      "\n",
      "Methods found: ['__init__']\n",
      "\n",
      " 1: class OpenTitanIngester:\n",
      " 2:     \"\"\"Handles ingestion of OpenTitan documentation and RTL\"\"\"\n",
      " 3:     \n",
      " 4:     def __init__(self, opentitan_root: str):\n",
      " 5:         self.opentitan_root = Path(opentitan_root)\n",
      " 6:         self.target_ips = ['uart', 'i2c', 'kmac', 'lc_ctrl', 'otbn', 'sysrst_ctrl']\n",
      " 7:         \n"
     ]
    }
   ],
   "source": [
    "# Analyze the EmbeddingManager class\n",
    "embedding_match = re.search(r'class EmbeddingManager:.*?(?=class [A-Z]|\\Z)', source_code, re.DOTALL)\n",
    "\n",
    "if embedding_match:\n",
    "    embedding_code = embedding_match.group(0)\n",
    "    \n",
    "    print(\"EmbeddingManager Class Analysis:\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    # Find methods\n",
    "    methods = re.findall(r'def (\\w+)\\(', embedding_code)\n",
    "    print(\"Methods:\")\n",
    "    for method in methods:\n",
    "        print(f\"- {method}\")\n",
    "    \n",
    "    # Check for model initialization\n",
    "    model_match = re.search(r'SentenceTransformer\\([^)]*\\)', embedding_code)\n",
    "    if model_match:\n",
    "        print(f\"\\nEmbedding model: {model_match.group(0)}\")\n",
    "    \n",
    "    # Check for FAISS usage\n",
    "    if 'faiss' in embedding_code:\n",
    "        print(\"Uses FAISS for vector indexing\")\n",
    "        \n",
    "    # Show search method\n",
    "    search_match = re.search(r'def search\\([^)]*\\):(.*?)(?=def|\\Z)', embedding_code, re.DOTALL)\n",
    "    if search_match:\n",
    "        print(\"\\nSearch method implementation found\")\n",
    "        \n",
    "else:\n",
    "    print(\"EmbeddingManager class not found in source code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b819d54",
   "metadata": {},
   "source": [
    "## 6. SVAGenerator Class\n",
    "\n",
    "The SVAGenerator class integrates with language models to generate SystemVerilog assertions. It processes RTL signal descriptions and documentation context to produce relevant assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5149066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingManager class analysis:\n",
      "\n",
      "Methods: ['__init__', 'build_faiss_index']\n",
      "\n",
      " 1: class EmbeddingManager:\n",
      " 2:     \"\"\"Manages embeddings and FAISS indexes\"\"\"\n",
      " 3:     \n",
      " 4:     def __init__(self, model_name: str = \"BAAI/bge-base-en-v1.5\"):\n",
      " 5:         self.model = SentenceTransformer(model_name)\n",
      " 6:         self.indexes = {}  # ip_name -> faiss index\n",
      " 7:         self.chunks = {}   # ip_name -> list of chunks\n",
      " 8:         \n",
      " 9:     def create_embeddings(self, chunks: List[DocumentChunk]) -> np.ndarray:\n",
      "10:         \"\"\"Create embeddings for document chunks\"\"\"\n",
      "11:         if not chunks:\n",
      "12:             return np.array([])\n",
      "13:         texts = [f\"{chunk.section}: {chunk.content}\" for chunk in chunks]\n",
      "14:         embeddings = self.model.encode(texts, normalize_embeddings=True)\n",
      "15:         return embeddings\n",
      "16:     \n",
      "17:     def build_faiss_index(self, ip_name: str, chunks: List[DocumentChunk]):\n",
      "\n",
      "Key Insights:\n",
      "  - Uses BAAI/bge-base-en-v1.5 model for embeddings\n",
      "  - FAISS enables fast similarity search\n",
      "  - Maintains separate indices per IP block\n",
      "  - Normalizes embeddings for better search quality\n"
     ]
    }
   ],
   "source": [
    "# Analyze the SVAGenerator class\n",
    "sva_match = re.search(r'class SVAGenerator:.*?(?=class [A-Z]|\\Z)', source_code, re.DOTALL)\n",
    "\n",
    "if sva_match:\n",
    "    sva_code = sva_match.group(0)\n",
    "    \n",
    "    print(\"SVAGenerator Class Analysis:\")\n",
    "    print(\"-\" * 27)\n",
    "    \n",
    "    # Find methods\n",
    "    methods = re.findall(r'def (\\w+)\\(', sva_code)\n",
    "    print(\"Methods:\")\n",
    "    for method in methods:\n",
    "        print(f\"- {method}\")\n",
    "    \n",
    "    # Check for model configuration\n",
    "    model_match = re.search(r'model_name\\s*=\\s*[\"\\']([^\"\\']+)', sva_code)\n",
    "    if model_match:\n",
    "        print(f\"\\nDefault model: {model_match.group(1)}\")\n",
    "    \n",
    "    # Find system prompt\n",
    "    system_prompt_match = re.search(r'system_prompt\\s*=\\s*[\"\\']([^\"\\']+)', sva_code)\n",
    "    if system_prompt_match:\n",
    "        print(\"System prompt configuration found\")\n",
    "        \n",
    "    # Check for assertion generation method\n",
    "    if 'generate_assertion' in sva_code:\n",
    "        print(\"Assertion generation method implemented\")\n",
    "        \n",
    "else:\n",
    "    print(\"SVAGenerator class not found in source code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2847519",
   "metadata": {},
   "source": [
    "## 7. OpenTitanSVASystem Integration\n",
    "\n",
    "The OpenTitanSVASystem class coordinates all components to provide a complete assertion generation pipeline. It manages the workflow from documentation ingestion to assertion output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74632ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVAGenerator class analysis:\n",
      "\n",
      "Methods: ['__init__', 'load_model']\n",
      "\n",
      "System Prompt (key instructions to the LLM):\n",
      "\n",
      "You are a world-class expert in SystemVerilog Assertions (SVA) for semiconductor IP verification. Your task is to generate precise, high-quality SVA properties based on the provided context.\n",
      "\n",
      "Follow these rules strictly:\n",
      "1.  **Property Block:** Enclose every assertion in a named `property` block.\n",
      "2.  **Assertion:** Follow each property with a corresponding `assert property` statement.\n",
      "3.  **Clocking:** Use `@(posedge clk_i)` for clocking.\n",
      "4.  **Reset:** Use `disable iff (!rst_ni)` for asynchronous reset.\n",
      "5.  **Comments:** Add a brief, insightful comment above each property explaining its purpose.\n",
      "6.  **No Placeholders:** Do not use placeholder signals. Only use signals found in the context.\n",
      "7.  **Focus:** Generate properties directly related to the user's query and the provided context.\n",
      "\n",
      "E...\n",
      "\n",
      "Prompt Engineering Insights:\n",
      "  - Establishes expert persona for the LLM\n",
      "  - Provides strict formatting rules\n",
      "  - Includes clocking and reset conventions\n",
      "  - Emphasizes using only real signals from context\n",
      "  - Shows example of perfect SVA property\n",
      "\n",
      "LLM Model: Qwen/Qwen2-7B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Analyze the main OpenTitanSVASystem class\n",
    "main_match = re.search(r'class OpenTitanSVASystem:.*?(?=if __name__|\\Z)', source_code, re.DOTALL)\n",
    "\n",
    "if main_match:\n",
    "    main_code = main_match.group(0)\n",
    "    \n",
    "    print(\"OpenTitanSVASystem Class Analysis:\")\n",
    "    print(\"-\" * 34)\n",
    "    \n",
    "    # Find methods\n",
    "    methods = re.findall(r'def (\\w+)\\(', main_code)\n",
    "    print(\"Methods:\")\n",
    "    for method in methods:\n",
    "        print(f\"- {method}\")\n",
    "    \n",
    "    # Check component integration\n",
    "    if 'OpenTitanIngester' in main_code and 'EmbeddingManager' in main_code and 'SVAGenerator' in main_code:\n",
    "        print(\"\\nIntegrates all three main components:\")\n",
    "        print(\"- OpenTitanIngester\")\n",
    "        print(\"- EmbeddingManager\") \n",
    "        print(\"- SVAGenerator\")\n",
    "    \n",
    "    # Show workflow methods\n",
    "    workflow_methods = [m for m in methods if any(keyword in m.lower() for keyword in ['process', 'generate', 'run'])]\n",
    "    if workflow_methods:\n",
    "        print(f\"\\nWorkflow methods: {', '.join(workflow_methods)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"OpenTitanSVASystem class not found in source code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d8ab2",
   "metadata": {},
   "source": [
    "## 8. Practical Demonstration\n",
    "\n",
    "This section demonstrates the system's functionality by processing actual OpenTitan documentation and generating sample assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90d546b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- OpenTitan SVA Generator ---\n",
      "Available IPs: uart, i2c, kmac, lc_ctrl, otbn, sysrst_ctrl\n",
      "\n",
      "Generating SVA properties... (this may take a moment)\n",
      "Loading cached data for i2c...\n",
      "Loading Qwen/Qwen2-7B-Instruct...\n",
      "Loading Qwen/Qwen2-7B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.44it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Interaction logged to cache/logs/i2c_20250828_013024.json\n",
      "\n",
      "================================================================================\n",
      "GENERATED SVA PROPERTIES:\n",
      "```systemverilog\n",
      "// Property: Ensure Start Condition is generated when transmitting\n",
      "property start_condition_for_transmission;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  FDATA.START |=> (fmt_flag_start_before_i || !fmt_flag_stop_after_i);\n",
      "endproperty\n",
      "assert_start_condition: assert property (start_condition_for_transmission);\n",
      "\n",
      "// Property: Ensure Stop Condition is generated when reading\n",
      "property stop_condition_for_reading;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  FDATA.STOP |=> (fmt_flag_stop_after_i && !fmt_flag_start_before_i);\n",
      "endproperty\n",
      "assert_stop_condition: assert property (stop_condition_for_reading);\n",
      "\n",
      "// Property: Address Acknowledgment is detected within the expected timing\n",
      "property address_ack_within_timing;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  AddrAck |=> #(TIMING3.THD_DAT + 1) 1'b1;\n",
      "endproperty\n",
      "assert_address_ack_timing: assert property (address_ack_within_timing);\n",
      "\n",
      "// Property: Data Transmission Timing - TX Data Acknowledgment occurs within specified timing\n",
      "property data_transmit_ack_within_timing;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  TxAck |=> #(TIMING2.THD_STA + 1) 1'b1;\n",
      "endproperty\n",
      "assert_data_transmit_ack_timing: assert property (data_transmit_ack_within_timing);\n",
      "\n",
      "// Property: No unexpected START condition due to signal instability\n",
      "property no_unexpected_start_from_sda_instability;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  ~sda_unstable_i |=> !intr_unexp_stop_o;\n",
      "endproperty\n",
      "assert_no_unexpected_start: assert property (no_unexpected_start_from_sda_instability);\n",
      "\n",
      "// Property: No unexpected STOP condition due to signal instability\n",
      "property no_unexpected_stop_from_sda_instability;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  ~sda_unstable_i |=> !stop_detect_o;\n",
      "endproperty\n",
      "assert_no_unexpected_stop: assert property (no_unexpected_stop_from_sda_instability);\n",
      "```\n",
      "================================================================================\n",
      "\n",
      " Successfully imported the system components!\n",
      "\n",
      " Supported IP Blocks:\n",
      "  - UART\n",
      "  - I2C\n",
      "  - KMAC\n",
      "  - LC_CTRL\n",
      "  - OTBN\n",
      "  - SYSRST_CTRL\n",
      "Interaction logged to cache/logs/i2c_20250828_013024.json\n",
      "\n",
      "================================================================================\n",
      "GENERATED SVA PROPERTIES:\n",
      "```systemverilog\n",
      "// Property: Ensure Start Condition is generated when transmitting\n",
      "property start_condition_for_transmission;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  FDATA.START |=> (fmt_flag_start_before_i || !fmt_flag_stop_after_i);\n",
      "endproperty\n",
      "assert_start_condition: assert property (start_condition_for_transmission);\n",
      "\n",
      "// Property: Ensure Stop Condition is generated when reading\n",
      "property stop_condition_for_reading;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  FDATA.STOP |=> (fmt_flag_stop_after_i && !fmt_flag_start_before_i);\n",
      "endproperty\n",
      "assert_stop_condition: assert property (stop_condition_for_reading);\n",
      "\n",
      "// Property: Address Acknowledgment is detected within the expected timing\n",
      "property address_ack_within_timing;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  AddrAck |=> #(TIMING3.THD_DAT + 1) 1'b1;\n",
      "endproperty\n",
      "assert_address_ack_timing: assert property (address_ack_within_timing);\n",
      "\n",
      "// Property: Data Transmission Timing - TX Data Acknowledgment occurs within specified timing\n",
      "property data_transmit_ack_within_timing;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  TxAck |=> #(TIMING2.THD_STA + 1) 1'b1;\n",
      "endproperty\n",
      "assert_data_transmit_ack_timing: assert property (data_transmit_ack_within_timing);\n",
      "\n",
      "// Property: No unexpected START condition due to signal instability\n",
      "property no_unexpected_start_from_sda_instability;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  ~sda_unstable_i |=> !intr_unexp_stop_o;\n",
      "endproperty\n",
      "assert_no_unexpected_start: assert property (no_unexpected_start_from_sda_instability);\n",
      "\n",
      "// Property: No unexpected STOP condition due to signal instability\n",
      "property no_unexpected_stop_from_sda_instability;\n",
      "  @(posedge clk_i) disable iff (!rst_ni)\n",
      "  ~sda_unstable_i |=> !stop_detect_o;\n",
      "endproperty\n",
      "assert_no_unexpected_stop: assert property (no_unexpected_stop_from_sda_instability);\n",
      "```\n",
      "================================================================================\n",
      "\n",
      " Successfully imported the system components!\n",
      "\n",
      " Supported IP Blocks:\n",
      "  - UART\n",
      "  - I2C\n",
      "  - KMAC\n",
      "  - LC_CTRL\n",
      "  - OTBN\n",
      "  - SYSRST_CTRL\n"
     ]
    }
   ],
   "source": [
    "# First import the main components\n",
    "try:\n",
    "\n",
    "    exec(open('opentitan_sva_generator.py').read())\n",
    "    print(\" Successfully imported the system components!\")\n",
    "    \n",
    "    # Show the supported IP blocks\n",
    "    print(\"\\n Supported IP Blocks:\")\n",
    "    supported_ips = ['uart', 'i2c', 'kmac', 'lc_ctrl', 'otbn', 'sysrst_ctrl']\n",
    "    for ip in supported_ips:\n",
    "        print(f\"  - {ip.upper()}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Import error (expected in demo environment): {str(e)[:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer_annotated (3.9.21)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
